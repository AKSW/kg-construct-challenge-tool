#!/usr/bin/env python3

import os
import sys
import json
import unittest
import requests
import warnings
import psycopg2
import pymysql
import subprocess
import docker
import atexit
import filecmp
from csv import DictReader
from glob import glob
from container import Container, ContainerManager
from rmlmapper import RMLMapper
from postgresql import PostgreSQL
from mysql import MySQL
from virtuoso import Virtuoso
from morphkgc import MorphKGC
from morphrdb import MorphRDB
from sdmrdfizer import SDMRDFizer
from ontop import OntopMaterialize, OntopVirtualize
from fuseki import Fuseki
from query import Query
from yarrrml import YARRRML
from executor import Executor
from time import sleep
from rdflib import Graph
from threading import Event
from collector import Collector, METRICS_FILE_NAME
from stats import Stats, METRICS_AGGREGATED_FILE_NAME, METRICS_SUMMARY_FILE_NAME
from logger import Logger, LOG_FILE_NAME

LOG_DIR = os.path.join(os.path.dirname(__file__), 'log')
DATA_DIR = os.path.join(os.path.dirname(__file__), 'data')
CONFIG_DIR = os.path.join(os.path.dirname(__file__), 'config')
CASE_INFO_FILE_NAME = 'case-info.txt'
CHECKPOINT_FILE_NAME = '.done'
EXIT_CODE_INTERRUPTED = -4
METRIC_TYPE_STOP = 'STOP'
METRIC_TYPE_START = 'START'
METRIC_TYPE_INIT = 'INIT'
METRIC_TYPE_MEASUREMENT = 'MEASUREMENT'
METRIC_TYPE_EXIT = 'EXIT'

manager = ContainerManager()

@atexit.register
def clean():
    if len(manager.list_all()) > 0:
        print('Stopping all containers before exiting...')
        manager.stop_all()

    for f in glob(f'{os.path.join(DATA_DIR, "shared")}/*'):
        # Keep original data and mappings
        if 'student' in f or 'mapping' in f or f.endswith('.q') \
           or f.endswith('.sql'):
            continue
        os.remove(f)

class UnitTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def tearDownClass(self):
        manager.stop_all()

    def test_docker_run(self):
        logger = Logger('test_docker_run', LOG_DIR, True)
        c = Container('nginx:alpine', 'test_docker_run', logger,
                      {'80/tcp': '8080/tcp'})
        self.assertTrue(c.run())
        sleep(5)
        r = requests.get('http://localhost:8080')
        self.assertEqual(r.status_code, 200)
        r.raise_for_status()
        c.stop()

    def test_docker_run_and_wait_for_log(self):
        logger = Logger('test_docker_run_and_wait_for_log', LOG_DIR, True)
        c = Container('nginx:alpine', 'test_docker_run_and_wait_for_log',
                      logger, {'80/tcp': '8081/tcp'})
        self.assertTrue(c.run_and_wait_for_log('start worker process'))
        r = requests.get('http://localhost:8081')
        self.assertEqual(r.status_code, 200)
        r.raise_for_status()
        c.stop()

    def test_docker_run_and_wait_for_exit(self):
        logger = Logger('test_docker_run_and_wait_for_exit', LOG_DIR, True)
        c = Container('alpine:edge', 'test_docker_run_and_wait_for_exit',
                      logger, {'80/tcp': '8082/tcp'})
        self.assertTrue(c.run_and_wait_for_exit('sleep 5'))
        c.stop()

    def test_docker_logs(self):
        logger = Logger('test_docker_logs', LOG_DIR, True)
        c = Container('nginx:alpine', 'test_docker_logs', logger,
                      {'80/tcp': '8083/tcp'})
        self.assertTrue(c.run_and_wait_for_log('start worker process'))
        self.assertIsNotNone(c.logs())
        self.assertTrue(len(c.logs()) > 0)
        c.stop()

    def test_postgresql(self):
        postgresql = PostgreSQL(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(postgresql.initialization())
        self.assertTrue(postgresql.wait_until_ready())
        connection = psycopg2.connect(host='localhost', database='db',
                                      user='root', password='root')
        cursor = connection.cursor()

        # Test valid query
        cursor.execute('SELECT 1;')

        # Test load CSV
        self.assertTrue(postgresql.load('student.csv', 'student'))
        cursor.execute('SELECT name, age FROM student;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Test invalid query
        with self.assertRaises(psycopg2.errors.UndefinedTable):
            cursor.execute('SELECT * FROM INVALID_TABLE;')
        connection.rollback()

        connection.close()
        postgresql.stop()

        # Check if the tables are really dropped
        postgresql = PostgreSQL(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(postgresql.wait_until_ready())
        connection = psycopg2.connect(host='localhost', user='root',
                                      password='root', database='db')
        cursor = connection.cursor()
        with self.assertRaises(psycopg2.errors.UndefinedTable):
            cursor.execute('SELECT name, age FROM student;')
        connection.rollback()

        # Check if we can now reload
        self.assertTrue(postgresql.load('student.csv', 'student'))
        cursor.execute('SELECT name, age FROM student;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Check SQL schema loading with 2 tables
        CSV_FILES = [('student.csv', 'student1'),('student.csv', 'student2')]
        self.assertTrue(postgresql.load_sql_schema('schema_postgresql.sql',
                                                   CSV_FILES))
        # Verify table 1
        cursor.execute('SELECT name, age FROM student1;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Verify table 2
        cursor.execute('SELECT name, age FROM student2;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        connection.close()
        postgresql.stop()

    def test_mysql(self):
        mysql = MySQL(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(mysql.initialization())
        self.assertTrue(mysql.wait_until_ready())
        connection = pymysql.connect(host='localhost', user='root',
                                     password='root', db='db')
        cursor = connection.cursor()

        # Test valid query
        cursor.execute('SELECT 1;')

        # Test load CSV
        self.assertTrue(mysql.load('student.csv', 'student'))
        cursor.execute('SELECT name, age FROM student;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Test invalid query
        with self.assertRaises(pymysql.err.ProgrammingError):
            cursor.execute('SELECT * FROM INVALID_TABLE;')

        # Close connection and stop container to drop all created tables
        connection.close()
        mysql.stop()

        # Check if the tables are really dropped
        mysql = MySQL(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(mysql.wait_until_ready())
        connection = pymysql.connect(host='localhost', user='root',
                                     password='root', db='db')
        cursor = connection.cursor()
        with self.assertRaises(pymysql.err.ProgrammingError):
            cursor.execute('SELECT name, age FROM student;')

        # Check if we can now reload
        self.assertTrue(mysql.load('student.csv', 'student'))
        cursor.execute('SELECT name, age FROM student;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Check SQL schema loading with 2 tables
        CSV_FILES = [('student.csv', 'student1'),('student.csv', 'student2')]
        self.assertTrue(mysql.load_sql_schema('schema_mysql.sql', CSV_FILES))

        # Recreate cursor because we dropped tables
        connection.close()
        connection = pymysql.connect(host='localhost', user='root',
                                     password='root', db='db')
        cursor = connection.cursor()

        # Verify table 1
        cursor.execute('SELECT name, age FROM student1;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Verify table 2
        cursor.execute('SELECT name, age FROM student2;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        connection.close()
        mysql.stop()

    def test_yarrrml_rml(self):
        yarrrml = YARRRML(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        yarrrml_file = 'mapping.yarrrml'

        # YARRRML to RML
        mapping_file = 'mapping_yarrrml.rml.ttl'
        self.assertTrue(yarrrml.transform_mapping(yarrrml_file, mapping_file,
                                                  False))
        yarrrml.stop()

    def test_yarrrml_r2rml(self):
        yarrrml = YARRRML(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        yarrrml_file = 'mapping.yarrrml'

        # YARRRML to R2RML
        mapping_file = 'mapping_yarrrml.r2rml.ttl'
        self.assertTrue(yarrrml.transform_mapping(yarrrml_file, mapping_file,
                                                  True))
        yarrrml.stop()

    def test_collector_metrics(self):
        NUMBER_OF_STEPS = 4
        RUN = 1
        SAMPLE_INTERVAL = 0.1
        directory = os.path.join(DATA_DIR, 'collector')
        results_run_path = os.path.join(directory, 'results', f'run_{RUN}')

        try:
            os.remove(os.path.join(results_run_path, CASE_INFO_FILE_NAME))
        except FileNotFoundError:
            pass

        collector = Collector(results_run_path, SAMPLE_INTERVAL,
                              NUMBER_OF_STEPS, RUN, LOG_DIR, False)
        sleep(15)
        collector.next_step()
        sleep(20)
        collector.stop()

        self.assertTrue(os.path.exists(os.path.join(results_run_path,
                                                    CASE_INFO_FILE_NAME)))

        self.assertTrue(os.path.exists(os.path.join(results_run_path,
                                                    METRICS_FILE_NAME)))
        with open(os.path.join(results_run_path, METRICS_FILE_NAME), 'r') as f:
            reader = DictReader(f)
            step = 0
            index = 0
            timestamp = -1.0
            for line in reader:
                i = int(line['index'])
                s = int(line['step'])
                t = float(line['timestamp'])

                if i > index:
                    index = i
                else:
                    self.assertTrue(False, 'Index must always increment')

                if s > step:
                    step = s
                self.assertTrue(s >= step, 'Steps must always increment')
                self.assertTrue(s <= NUMBER_OF_STEPS,
                                'Steps must be <= than number of steps')

                if t > timestamp:
                    timestamp = t
                else:
                    print(t, s, i)
                    self.assertTrue(False, 'Timestamp must always increment')


    def test_stats_generation(self):
        metrics_aggregated_file = os.path.join(DATA_DIR, 'stats', 'results',
                                               METRICS_AGGREGATED_FILE_NAME)
        metrics_summary_file = os.path.join(DATA_DIR, 'stats', 'results',
                                            METRICS_SUMMARY_FILE_NAME)
        metrics_aggregated_expected_file = os.path.join(DATA_DIR,
                                                        'stats',
                                                        'expected',
                                                        METRICS_AGGREGATED_FILE_NAME)
        metrics_summary_expected_file = os.path.join(DATA_DIR,
                                                     'stats',
                                                     'expected',
                                                     METRICS_SUMMARY_FILE_NAME)
        try:
            os.remove(metrics_aggregated_file)
        except FileNotFoundError:
            pass

        try:
            os.remove(metrics_summary_file)
        except FileNotFoundError:
            pass

        stats = Stats(os.path.join(DATA_DIR, 'stats', 'results'), 4, LOG_DIR,
                      False)
        stats.aggregate()
        self.assertTrue(os.path.exists(metrics_aggregated_file))

        f1 = metrics_aggregated_file
        f2 = metrics_aggregated_expected_file
        self.assertTrue(filecmp.cmp(f1, f2, shallow=False))

        f1 = metrics_summary_file
        f2 = metrics_summary_expected_file
        self.assertTrue(filecmp.cmp(f1, f2, shallow=False))


class FileTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def tearDownClass(self):
        manager.stop_all()

    def test_rmlmapper_file(self):
        rmlmapper = RMLMapper(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'rmlmapper_file.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(rmlmapper.execute_mapping('mapping.rml.ttl',
                                                  'rmlmapper_file.nt',
                                                  'ntriples'))
        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'rmlmapper_file.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'rmlmapper_file.nt'),
                format='ntriples')
        # 3 triples and 1 mapped as NULL <predicate> NULL because NULL values 
        # are undefined in CSV.
        self.assertEqual(len(g), 4)
        rmlmapper.stop()

    def test_morphkgc_file(self):
        morphkgc = MorphKGC(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'morphkgc', 'shared',
                                   'morphkgc_file.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(morphkgc.execute_mapping('mapping.rml.ttl',
                                                 'morphkgc_file.nt',
                                                 'ntriples'))
        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'morphkgc_file.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'morphkgc_file.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)
        morphkgc.stop()

    def test_sdmrdfizer_file(self):
        sdmrdfizer = SDMRDFizer(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'sdmrdfizer_file.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(sdmrdfizer.execute_mapping('mapping.rml.ttl',
                                                   'sdmrdfizer_file.nt',
                                                   'ntriples'))
        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'sdmrdfizer_file.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared',
                             'sdmrdfizer_file.nt'), format='ntriples')
        self.assertEqual(len(g), 3)
        sdmrdfizer.stop()


class RDBTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def setUpClass(cls):
        cls._mysql = MySQL(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        cls._mysql.wait_until_ready()
        cls._mysql.load('student.csv', 'student')

        cls._postgresql = PostgreSQL(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        cls._postgresql.wait_until_ready()
        cls._postgresql.load('student.csv', 'student')

    @classmethod
    def tearDownClass(cls):
        cls._mysql.stop()
        cls._postgresql.stop()
        manager.stop_all()

    def test_rmlmapper_mysql(self):
        rmlmapper = RMLMapper(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'rmlmapper_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(rmlmapper.execute_mapping('mapping.r2rml.ttl',
                                                  'rmlmapper_mysql.nt',
                                                  'ntriples', 'root', 'root',
                                                  'MySQL', '3306', 'db',
                                                  'MySQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'rmlmapper_mysql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'rmlmapper_mysql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        rmlmapper.stop()

    def test_rmlmapper_postgresql(self):
        rmlmapper = RMLMapper(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'rmlmapper_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(rmlmapper.execute_mapping('mapping.r2rml.ttl',
                                                  'rmlmapper_postgresql.nt',
                                                  'ntriples', 'root', 'root',
                                                  'PostgreSQL', '5432', 'db',
                                                  'PostgreSQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'rmlmapper_postgresql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'rmlmapper_postgresql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        rmlmapper.stop()

    def test_morphkgc_mysql(self):
        morphkgc = MorphKGC(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'morphkgc_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(morphkgc.execute_mapping('mapping.r2rml.ttl',
                                                 'morphkgc_mysql.nt',
                                                 'ntriples', 'root', 'root',
                                                 'MySQL', '3306', 'db',
                                                 'MySQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'morphkgc_mysql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'morphkgc_mysql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        morphkgc.stop()

    def test_morphkgc_postgresql(self):
        morphkgc = MorphKGC(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'morphkgc_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(morphkgc.execute_mapping('mapping.r2rml.ttl',
                                                 'morphkgc_postgresql.nt',
                                                 'ntriples', 'root', 'root',
                                                 'PostgreSQL', '5432', 'db',
                                                 'PostgreSQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'morphkgc_postgresql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'morphkgc_postgresql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        morphkgc.stop()

    def test_sdmrdfizer_mysql(self):
        sdmrdfizer = SDMRDFizer(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'sdmrdfizer_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(sdmrdfizer.execute_mapping('mapping.r2rml.ttl',
                                                   'sdmrdfizer_mysql.nt',
                                                   'ntriples', 'root', 'root',
                                                   'MySQL', '3306', 'db',
                                                   'MySQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'sdmrdfizer_mysql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'sdmrdfizer_mysql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        sdmrdfizer.stop()

    def test_sdmrdfizer_postgresql(self):
        sdmrdfizer = SDMRDFizer(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'sdmrdfizer_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(sdmrdfizer.execute_mapping('mapping.r2rml.ttl',
                                                   'sdmrdfizer_postgresql.nt',
                                                   'ntriples', 'root', 'root',
                                                   'PostgreSQL', '5432', 'db',
                                                   'PostgreSQL'))

        exist = os.path.exists(os.path.join(DATA_DIR, 'shared',
                                            'sdmrdfizer_postgresql.nt'))
        self.assertTrue(exist)
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'sdmrdfizer_postgresql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        sdmrdfizer.stop()

    def test_ontopmaterialize_mysql(self):
        ontop = OntopMaterialize(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'ontopmaterialize_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(ontop.execute_mapping('mapping.r2rml.ttl',
                                              'ontopmaterialize_mysql.nt',
                                              'ntriples', 'root', 'root',
                                              'MySQL', '3306', 'db', 'MySQL'))

        exist = os.path.exists(os.path.join(DATA_DIR, 'shared',
                                            'ontopmaterialize_mysql.nt'))
        self.assertTrue(exist)
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'ontopmaterialize_mysql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        ontop.stop()

    def test_ontopmaterialize_postgresql(self):
        ontop = OntopMaterialize(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'ontopmaterialize_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(ontop.execute_mapping('mapping.r2rml.ttl',
                                              'ontopmaterialize_postgresql.nt',
                                              'ntriples', 'root', 'root',
                                              'PostgreSQL', '5432', 'db',
                                              'PostgreSQL'))

        exist = os.path.exists(os.path.join(DATA_DIR, 'shared',
                                            'ontopmaterialize_postgresql.nt'))
        self.assertTrue(exist)
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared',
                             'ontopmaterialize_postgresql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        ontop.stop()

    def test_ontopendpoint_mysql(self):
        QUERY='PREFIX foaf: <http://xmlns.com/foaf/0.1/> ' + \
              'CONSTRUCT WHERE { ?s foaf:name ?o1 . }'
        ontop = OntopVirtualize(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'ontopendpoint_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(ontop.execute_mapping('mapping.r2rml.ttl',
                                              'ontopendpoint_mysql.nt',
                                              'ntriples', 'root', 'root',
                                              'MySQL', '3306', 'db', 'MySQL'))
        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q._execute(QUERY, ontop.endpoint, ontop.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 10, str(results))

        ontop.stop()

    def test_ontopendpoint_postgresql(self):
        QUERY='PREFIX foaf: <http://xmlns.com/foaf/0.1/> ' + \
              'CONSTRUCT WHERE { ?s foaf:name ?o1 . }'
        ontop = OntopVirtualize(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'ontopendpoint_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(ontop.execute_mapping('mapping.r2rml.ttl',
                                              'ontopendpoint_postgresql.nt',
                                              'ntriples', 'root', 'root',
                                              'PostgreSQL', '5432', 'db',
                                              'PostgreSQL'))
        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q._execute(QUERY, ontop.endpoint, False, ontop.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3, str(results))

        ontop.stop()

    def test_morphrdb_mysql(self):
        morphrdb = MorphRDB(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'morphrdb_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(morphrdb.execute_mapping('mapping.r2rml.ttl',
                                                 'morphrdb_mysql.nt',
                                                 'ntriples', 'root', 'root',
                                                 'MySQL', '3306', 'db',
                                                 'MySQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'morphrdb_mysql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'morphrdb_mysql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3, str(g))

        morphrdb.stop()

    def test_morphrdb_postgresql(self):
        morphrdb = MorphRDB(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'morphrdb_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(morphrdb.execute_mapping('mapping.r2rml.ttl',
                                                 'morphrdb_postgresql.nt',
                                                 'ntriples', 'root', 'root',
                                                 'PostgreSQL', '5432', 'db',
                                                 'PostgreSQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'morphrdb_postgresql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'morphrdb_postgresql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        morphrdb.stop()

class TriplestoreTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def tearDownClass(cls):
        manager.stop_all()

    def test_virtuoso(self):
        QUERY='PREFIX foaf: <http://xmlns.com/foaf/0.1/> ' + \
              'CONSTRUCT WHERE { ?s foaf:name ?o1 . }'
        virtuoso = Virtuoso(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(virtuoso.initialization())
        self.assertTrue(virtuoso.wait_until_ready())

        # Check if web interface is up
        r = requests.get('http://localhost:8890')
        self.assertEqual(r.status_code, 200, f'HTTP status {r.status_code}')
        r.raise_for_status()

        # Check if SPARQL endpoint works
        r = requests.get('http://localhost:8890/sparql/'
                         '?default-graph-uri=&query=CONSTRUCT+WHERE+'
                         '%7B%0D%0A++%3Fs+%3Fp+%3Fo.%0D%0A%7D%0D%0ALIMIT+100'
                         '&format=text%2Fplain')
        self.assertEqual(r.status_code, 200)
        r.raise_for_status()

        # Check if iSQL is up, HTTP is unsupported on the iSQL port
        # so the connection will be closed without a response
        with self.assertRaises(requests.exceptions.ConnectionError) as e:
            r = requests.get('http://localhost:1111')
            r.raise_for_status()
        self.assertTrue('Connection aborted' in str(e.exception))

        # Test load RDF
        self.assertTrue(virtuoso.load('student.nt'))

        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q._execute(QUERY, virtuoso.endpoint, False, virtuoso.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3, str(results))

        # RDF is dropped when container is stopped
        virtuoso.stop()

        # Virtuoso is already initialized only wait for it to be ready
        virtuoso = Virtuoso(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(virtuoso.wait_until_ready())

        # Verify removed data
        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q._execute(QUERY, virtuoso.endpoint, True, virtuoso.headers)
        self.assertIsNone(results, str(results))

        # Test if we can now reload our RDF
        self.assertTrue(virtuoso.load_parallel('stude*.nt', 4))

        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q._execute(QUERY, virtuoso.endpoint, virtuoso.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3, str(results))

        virtuoso.stop()

    def test_fuseki(self):
        QUERY='PREFIX foaf: <http://xmlns.com/foaf/0.1/> ' + \
              'CONSTRUCT WHERE { ?s foaf:name ?o1 . }'
        fuseki = Fuseki(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(fuseki.initialization())
        self.assertTrue(fuseki.wait_until_ready())

        # Load RDF
        self.assertTrue(fuseki.load('student.nt'))

        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q._execute(QUERY, fuseki.endpoint, fuseki.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3, str(results))

        fuseki.stop()

        # Verify removed data
        fuseki = Fuseki(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(fuseki.wait_until_ready())

        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q._execute(QUERY, fuseki.endpoint, True, fuseki.headers)
        self.assertIsNone(results, str(results))

        # Test if we can now reload our RDF
        self.assertTrue(fuseki.load('student.nt'))

        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q._execute(QUERY, fuseki.endpoint, False, fuseki.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3, str(results))

        fuseki.stop()

    def test_query_execute(self):
        QUERY = 'CONSTRUCT WHERE { ?s ?p ?o. } LIMIT 100'
        DBPEDIA_SPARQL = 'https://dbpedia.org/sparql'

        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q._execute(QUERY, DBPEDIA_SPARQL, False)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 100)

    def test_query_execute_and_save(self):
        QUERY = 'CONSTRUCT WHERE { ?s ?p ?o. } LIMIT 100'
        DBPEDIA_SPARQL = 'https://dbpedia.org/sparql'

        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(q.execute_and_save(QUERY, DBPEDIA_SPARQL, 'results.nt',
                                           {}))
        with open(os.path.join(DATA_DIR, 'shared', 'results.nt'), 'r') as f:
            results = list(filter(None, f.read().split('\n')))
            self.assertEqual(len(results), 100)

    def test_query_execute_from_file(self):
        QUERY_FILE = 'spo100.q'
        DBPEDIA_SPARQL = 'https://dbpedia.org/sparql'

        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        results = q.execute_from_file(QUERY_FILE, DBPEDIA_SPARQL, {})
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 100, str(results))

    def test_query_execute_from_file_and_save(self):
        QUERY_FILE = 'spo100.q'
        DBPEDIA_SPARQL = 'https://dbpedia.org/sparql'

        q = Query(DATA_DIR, CONFIG_DIR, LOG_DIR, False)
        self.assertTrue(q.execute_from_file_and_save(QUERY_FILE, DBPEDIA_SPARQL,
                                                     'results.nt', {}))
        with open(os.path.join(DATA_DIR, 'shared', 'results.nt'), 'r') as f:
            results = list(filter(None, f.read().split('\n')))
            self.assertEqual(len(results), 100)

class IntegrationTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def tearDownClass(cls):
        manager.stop_all()

    def test_executor_list(self):
        executor = Executor(os.path.join(DATA_DIR, 'test-cases'), False)
        cases = executor.list()
        iris = [x['data']['@id'] for x in cases]
        BASE_URL = 'http://example.com/test-cases'
        e = [f'{BASE_URL}/rmlmapper/fuseki/transform-csv-to-ntriples',
             f'{BASE_URL}/rmlmapper/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/rmlmapper/fuseki/transform-postgresql-to-ntriples',
             f'{BASE_URL}/rmlmapper/virtuoso/transform-csv-to-ntriples',
             f'{BASE_URL}/rmlmapper/virtuoso/transform-mysql-to-ntriples',
             f'{BASE_URL}/rmlmapper/virtuoso/transform-postgresql-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/fuseki/transform-csv-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/fuseki/transform-postgresql-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/virtuoso/transform-csv-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/virtuoso/transform-mysql-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/virtuoso/transform-postgresql-to-ntriples',
             f'{BASE_URL}/morphkgc/fuseki/transform-csv-to-ntriples',
             f'{BASE_URL}/morphkgc/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/morphkgc/fuseki/transform-postgresql-to-ntriples',
             f'{BASE_URL}/morphkgc/virtuoso/transform-csv-to-ntriples',
             f'{BASE_URL}/morphkgc/virtuoso/transform-mysql-to-ntriples',
             f'{BASE_URL}/morphkgc/virtuoso/transform-postgresql-to-ntriples',
             f'{BASE_URL}/morphrdb/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/morphrdb/fuseki/transform-postgresql-to-ntriples',
             f'{BASE_URL}/morphrdb/virtuoso/transform-mysql-to-ntriples',
             f'{BASE_URL}/morphrdb/virtuoso/transform-postgresql-to-ntriples',
             f'{BASE_URL}/ontopmaterialize/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/ontopmaterialize/fuseki/'
             'transform-postgresql-to-ntriples',
             f'{BASE_URL}/ontopmaterialize/virtuoso/'
             'transform-mysql-to-ntriples',
             f'{BASE_URL}/ontopmaterialize/virtuoso/'
             'transform-postgresql-to-ntriples',
             f'{BASE_URL}/ontopendpoint/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/ontopendpoint/fuseki/'
             'transform-postgresql-to-ntriples']
        # We expect 28 cases
        self.assertEqual(len(iris), 28)
        self.assertListEqual(sorted(e), sorted(iris))

    def test_executor_run(self):
        path = os.path.join(DATA_DIR, 'test-cases')
        executor = Executor(path, True)

        for case in executor.list():
            # Clean case
            executor.clean(case)

            # Check if all files are removed
            for step in case['data']['steps']:
                directory = case['directory']
                resource = step['resource'].lower().replace('_', '')
                log_file = os.path.join(directory, 'results', 'run_1', resource,
                                        LOG_FILE_NAME)
                checkpoint_file = os.path.join(directory, CHECKPOINT_FILE_NAME)
                metrics_file = os.path.join(directory, 'results', 'run_1',
                                            resource, METRICS_FILE_NAME)

                self.assertFalse(os.path.exists(log_file))
                self.assertFalse(os.path.exists(metrics_file))
                self.assertFalse(os.path.exists(checkpoint_file))

            # Run case
            self.assertTrue(executor.run(case, 0.1, 1, True))

            # Check if all files are generated
            for step in case['data']['steps']:
                directory = case['directory']
                resource = step['resource'].lower().replace('_', '')
                log_file = os.path.join(directory, 'results', 'run_1',
                                        LOG_FILE_NAME)
                checkpoint_file = os.path.join(directory, CHECKPOINT_FILE_NAME)
                metrics_file = os.path.join(directory, 'results', 'run_1',
                                            METRICS_FILE_NAME)

                # Verify if log, checkpoint, metrics files exists
                # Log files are expected to contain at least 1 line,
                # Metrics files must exist, the content is validated in 
                # unittests and checkpoint files contain a single line
                # with the timestamp
                self.assertTrue(os.path.exists(log_file),
                                f'{log_file} does not exist')
                with open(log_file) as f:
                    self.assertTrue(len(f.readlines()) > 0)
                self.assertTrue(os.path.exists(metrics_file),
                                f'{metrics_file} does not exist')
                self.assertTrue(os.path.exists(checkpoint_file),
                                f'{checkpoint_file} does not exist')
                with open(checkpoint_file) as f:
                    self.assertEqual(len(f.readlines()), 1)

if __name__ == '__main__':
    # SELinux causes weird permission denied issues, warn users
    try:
        response = subprocess.check_output('getenforce')
        if response.decode().strip() != 'Permissive':
            print('SELinux must be set to "permissive" to allow containers '
                  'accessing files in mounted directories', file=sys.stderr)
            sys.exit(-1)
    except subprocess.CalledProcessError:
        pass
    except FileNotFoundError:
        pass

    # Allow stderr logging to be silent during tests, logs are available
    # in the log file
    os.environ['UNITTEST'] = '1'

    if len(manager.list_all()) != 0:
        print('Stopping all containers before starting tests...')
        manager.stop_all()

    if len(sys.argv) > 1:
        unittest.main()
    else:
        print('*' * 3 + ' UNIT TESTS ' + '*' * 3)
        unittest.main(defaultTest='UnitTests', exit=False)
        print('*' * 3 + ' FILE TESTS ' + '*' * 3)
        unittest.main(defaultTest='FileTests', exit=False)
        print('*' * 3 + ' RDB TESTS ' + '*' * 3)
        unittest.main(defaultTest='RDBTests', exit=False)
        print('*' * 3 + ' TRIPLE STORE TESTS ' + '*' * 3)
        unittest.main(defaultTest='TriplestoreTests', exit=False)
        print('*' * 3 + 'INTEGRATION TESTS' + '*' * 3)
        unittest.main(defaultTest='IntegrationTests')
